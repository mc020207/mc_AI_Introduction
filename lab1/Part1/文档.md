<center><big><big><big><big><big><big>Lab1 Part1实验报告</big></big></big></big></big></big></center>

<center><big><big><big>马成 20307130112</big></big></big></center>

## 代码基本架构

​		整个工程大致分成了4个部分完成，`init.py`是对初始数据的简单处理，将数据进行处理之后分成train validate test三个部分用`.npy`的格式存储。`main.py`主要是做与网路训练有关的准备，`test.py`是用于网络效果的检验。`net.py`是工程的核心部分，下面将展示一些核心部分，初始化等部分会直接省略。由于Part1两个小实验的代码过于接近，我这里就放在一起展示，同时由于实验二所需要的东西更加丰富，下面对于代码的展示以实验二的部分为主。

### 大致逻辑

​		第一个实验就是先通过若干层forward计算出每一种汉字的预测x对应的y值，第二个实验就是先通过若干层forward计算出每一种汉字的预测可能性，随后根据各自损失函数计算每一层网络中每一个参数的梯度存储在网络中，最后统一用一个函数进行调整。

### 线性层

```python
class Linear():
    def forward(self,inputs):
        self.inputs=inputs
        outputs=np.matmul(self.inputs,self.params['W'])+self.params['b']
        return outputs

    def backward(self,grads):
        self.grads['W']=np.matmul(self.inputs.T,grads)
        self.grads['b']=np.sum(grads,axis=0)
        return np.matmul(grads,self.params['W'].T)
```

### 非线性层

```python
class Logistic():
    def forward(self,inputs):
        outputs=1.0/(1.0+np.exp(-inputs))
        self.outputs=outputs
        return outputs

    def backward(self,grads):
        outputs_grad_inputs=np.multiply(self.outputs,(1.0-self.outputs))
        return np.multiply(grads,outputs_grad_inputs)
```

### softmax+线性层

```python
class softmaxLinear():
    def forward(self,inputs):
        self.inputs=inputs
        outputs=np.matmul(self.inputs,self.params['W'])+self.params['b']
        self.outputs=self.softmax(outputs)
        return self.outputs

    def softmax(self,X):
        X_max=np.max(X,axis=1,keepdims=True)
        X_new=np.exp(X-X_max)
        return X_new/(np.sum(X_new,axis=1,keepdims=True))

    def backward(self,labels):
        N =labels.shape[0]
        labels =np.eye(12)[labels]
        self.grads['W']=-1/N*np.matmul(self.inputs.T,(labels-self.outputs))
        self.grads['b']=-1/N*np.matmul(np.ones([1,N],dtype=np.float64),
                                       (labels-self.outputs))
        return np.matmul(self.outputs-labels,self.params['W'].T)

```

这里之所以将softmax函数和最后一层的线性层写在一起是因为发现了最后一层线性层、softmax和最后的损失函数结合在一起计算反向传播的求导比较简单。

### 损失函数

损失函数使用的交叉熵损失函数

```python
class MultiCrossEntropyLoss():
    def forward(self,predicts,labels):
        self.predicts=predicts
        self.labels=labels
        self.num=self.predicts.shape[0]
        loss=0
        for i in range(self.num):
            index=self.labels[i]
            loss-=np.log(self.predicts[i][int(index)])
        return loss/self.num

    def backward(self):
        loss_grad_predicts=-(self.labels/self.predicts-
                             (1-self.labels)/(1-self.predicts))/self.num
        self.model.backward(loss_grad_predicts)
```

### Runner

这里写一个Runner类用于管理训练的过程以及手动处理batch的输入等

```python
def train(self,train_set,dev_set,**kwargs):
        num_epochs=kwargs.get("num_epochs",0)
        log_epochs=kwargs.get("log_epochs",100)
        save_dir=kwargs.get("save_dir",None)
        best_score=self.evaluate(dev_set)
        X_train,y_train=train_set
        X_num=X_train.shape[0]
        for epoch in range(num_epochs):
            X_train,y_train=train_set
            c=list(zip(X_train,y_train))
            random.shuffle(c)
            X_train,y_train=zip(*c)
            batches=[(X_train[i:min(X_num,i+64)],y_train[i:min(X_num,i+64)]) for i in range(0,X_num-64,64)]
            for (X,y) in batches:
                X=np.array(X)
                y=np.array(y)
                logits=self.model(X)
                train_loss=self.loss_fn(logits,y)
                train_score=self.metric(logits,y).item()
                self.model.backward(y)
                self.optimizer.step()
            dev_score=self.evaluate(dev_set)
            if dev_score > best_score:
                print(f"[Evaluate] params update: {best_score:.5f} --> {dev_score:.5f}")
                best_score=dev_score
                if save_dir:
                    self.save_model(save_dir)
            if log_epochs and epoch % log_epochs == 0:
                print(f"[Train] epoch: {epoch}/{num_epochs},train_loss: {train_loss.item()},train_score: {train_score}")
```

## 当前神经网络结构

### 实验一

1. 1到32的线性层
2. Logistic层
3. 32到16的线性层
4. Logistic层
5. 16到8的线性层
6. Logistic层
7. 8到1的线性层

### 实验二

1. 28*28到1024的线性层
2. Logistic层
3. 1024到2048的线性层
4. Logistic层
5. 2048到512的线性层
6. Logistic层
7. 512到12的线性层
8. softmax层

## 不同网络结构、网络参数的实验比较

1. 曾经尝试过再添加一层神经网络，但是实验后发现再添加一层之后训练速度大幅度减慢而且感觉变得不好训练了，大致训练了50个epoch准确度还是只能达到30%左右，效果远不如当前神经网络
2. 之前尝试的是将线性层的w设置为0到1的均匀随机数，b初始设置为0，发现训练效果不佳，大致只能到达75%左右的准确率，使用了老师上课时候提及的设定初始值方法，准确率大幅度提升
   1. W的初始化为[-1,1]的随机数/sqrt(连接的神经元个数)
   2. b的初始化为[-0.2,+0.2]最后一层 其他是[-1,0]
   3. lr=0.05 网络越大参数越多lr越小
   4. 中间层大约为输入的1.5倍左右
   5. 考虑mini batch (30-50)
3. 对于实验1我没有做太多的探索，感觉比较简单

## 对反向传播算法的理解

​		实验一的任务是拟合一个函数，其实这个分类任务的本质也是通过全连接神经网络来拟合一个函数，这个函数的输入是28*28的图片输出是12个值分别表示每一个汉字的可能性。神经网络拥有大规模的参数，通过神经网络可以拟合所有的函数重点就是希望通过调整参数来尽可能的接近正确的函数。

​		那么反向传播算法就是通过当前参数计算出一个结果，根据损失函数定量的比较当前计算出来的结果和希望得到的结果之间的差别，利用这个多层网络导数使得神经网络的参数按照梯度方向变化从而尽可能是下一次计算得到的结果更加靠近预期值。

​		多层神经网络的汇聚其实本质上就是若干个映射的复合，那么根据复合函数求导的运算规则，将梯度一层一层的反向传递下去给每一层修改参数，最终就可以得到一个较好的拟合。

## 模型方法（在两个实验中比较类似）

### 训练

1. 设定参数保存的文件夹的名字（`save_path="mpl_best_model3.pdparams"`），如果文件夹已经存在，模型将会在一开始读取这个文件夹中的参数，并再次基础上训练
2. 直接运行`main.py`即可，中途会给出一些训练参量
3. `obj.fit(X_train,y_train,X_validate,y_validate,lr=0.02,num_epochs=5000,log_epochs=100,save_path=save_path)`
   1. 要求给出本次训练的训练集、验证集、学习率
   2. `num_epochs`表示需要训练的轮数
   3. `log_epochs`表示每训练多少轮会输出一次详细信息

### 测试

1. 设定希望读取的文件夹名字（`save_path="mpl_best_model3.pdparams"`）
2. `obj.predict(X_test,y_test,save_path)`给出测试集即可

### 修改模型结构

1. 修改每一层的神经元数量`self.model=Model_MLP_L2(input_size=input_dim,output_size=output_dim,hidden_size1=32,hidden_size2=16,hidden_size3=8)`直接在定义模型的时候更改每一个隐藏层的大小即可

2. 修改模型参数

   ```python
   class Model_MLP_L2():
       def __init__(self, input_size, 
                    hidden_size1,hidden_size2,hidden_size3,output_size):
           self.fc1 = Linear(input_size, hidden_size1, name="fc1")
           self.act_fn1 = Logistic()
           self.fc2 = Linear(hidden_size1, hidden_size2, name="fc2")
           self.act_fn2 = Logistic()
           self.fc3=Linear(hidden_size2,hidden_size3,name="fc3")
           self.act_fn3=Logistic()
           self.fc4=Linear(hidden_size3,output_size,name="fc4")
           self.layers = [self.fc1, self.act_fn1,self.fc2,self.act_fn2,self.fc3,self.act_fn3,self.fc4]
   ```

   - 在定义模型的时候注意增减参数规模。
   - 定义你希望搭建的网络，并在Layer中放入所有网络。
   - 修改完成，运行即可